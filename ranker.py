# File: ranker.py
import os
import json
import subprocess


def query_ollama(prompt, model="llama3.1:8b"):
    result = subprocess.run(
        ["ollama", "run", model],
        input=prompt,
        capture_output=True,
        text=True,
        encoding="utf-8",
        errors="replace"
    )
    return result.stdout.strip()


def rate_clips(srt_folder, output_file="clip_ratings.json"):
    results = []

    for file in os.listdir(srt_folder):
        if file.endswith(".srt"):
            srt_path = os.path.join(srt_folder, file)

            with open(srt_path, "r", encoding="utf-8") as f:
                subtitles = f.read()

            prompt = f"""
You are a content critic. Given the subtitles of a short video clip, 
rate how entertaining it is for a general TikTok audience. Remember, TikTok viewers have short attention spans and prefer engaging, funny, or surprising content.
Do not rate clips highly if they don't make sense without context. Don't rate clips highly if anyone uses slurs or offensive language. (glorifying violence etc)
I want the "setup" and "punchline" of a joke to be in the same clip.
Subtitles of the clip:
---
{subtitles}
---

Respond in JSON only with two fields:
- "rating" (float 1.00-100.00, 100.00 = most entertaining)
- "reason" (short explanation, one sentence max)
"""

            print(f"Querying LLM for {file}...")
            response = query_ollama(prompt)

            try:
                data = json.loads(response)
            except json.JSONDecodeError:
                data = {"rating": None, "reason": response}

            results.append({"clip": file, **data})

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"Ratings saved to {output_file}")


def _srt_to_text_with_timestamps(srt_content):
    """Parse a simple SRT content into a list of (start_seconds, end_seconds, text).

    This is a lightweight parser that expects the common SRT blocks:
    index\nstart --> end\ntext\n\n
    Not a full spec parser, but good enough for short transcripts generated by this project.
    """
    import re

    entries = []
    blocks = re.split(r"\n\s*\n", srt_content.strip())
    time_re = re.compile(r"(\d{2}):(\d{2}):(\d{2}),(\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2}),(\d{3})")

    def to_seconds(h, m, s, ms):
        return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000.0

    for block in blocks:
        lines = block.strip().splitlines()
        if len(lines) >= 2:
            # first line could be index
            if time_re.search(lines[0]):
                time_line = lines[0]
                text_lines = lines[1:]
            else:
                time_line = lines[1]
                text_lines = lines[2:]

            m = time_re.search(time_line)
            if not m:
                continue

            start = to_seconds(m.group(1), m.group(2), m.group(3), m.group(4))
            end = to_seconds(m.group(5), m.group(6), m.group(7), m.group(8))
            text = " ".join(l.strip() for l in text_lines if l.strip())
            entries.append((start, end, text))

    return entries


def rate_full_transcript(srt_path, output_file="clip_ratings.json", model="llama3.1:8b"):
    """Read an entire SRT transcript and ask the LLM to pick the single most entertaining moment.

    Behavior:
    - Loads the SRT file as one transcript with timestamps.
    - Sends the transcript to the LLM and asks it to reply with a JSON object containing:
        - "best_timestamp" (seconds, integer) representing the recommended start time for a short clip
        - "window_seconds" (int) how long the clip should be (suggested 10-20)
        - "reason" (one-sentence)

    The prompt instructs the model to pick a timestamp that is within 5-15 seconds of a minute mark
    (e.g., near 00:01:00, 00:02:00, etc.) when possible, to better align with platform-friendly minute boundaries.

    The function writes a single JSON object to `output_file`.
    """
    with open(srt_path, "r", encoding="utf-8") as f:
        srt_content = f.read()

    entries = _srt_to_text_with_timestamps(srt_content)

    # Flatten transcript into a single text blob but keep markers for timestamps every 30-60s for the model.
    flattened = []
    for start, end, text in entries:
        flattened.append(f"[{int(start)}-{int(end)}] {text}")

    transcript_blob = "\n".join(flattened)

    prompt = f"""
You are a content critic that only responds in JSON. Given a full video transcript with timestamps, pick the single best moment
that would make the most entertaining short clip for a general TikTok audience. TikTok viewers prefer engaging, funny, or surprising
moments. Prefer moments that are near minute marks to help editing logistics: choose a timestamp within 5-15 seconds before or after
an exact minute (for example 00:01:05 or 00:00:55 is acceptable for the 1 minute mark). If no good moment falls near a minute mark,
you may pick any other timestamp.

Transcript (format: [start-end] text):
---
{transcript_blob}
---

Respond in JSON only with these fields:
- "best_timestamp": integer seconds where a clip should start
- "window_seconds": integer duration in seconds for the clip (suggest 8-20)
- "reason": one sentence explaining why
"""

    print(f"Querying LLM for full transcript {os.path.basename(srt_path)}...")
    response = query_ollama(prompt, model=model)

    try:
        data = json.loads(response)
    except json.JSONDecodeError:
        data = {"best_timestamp": None, "window_seconds": None, "reason": response}

    output = {"transcript_file": os.path.basename(srt_path), **data}

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)

    print(f"Full-transcript rating saved to {output_file}")

    return output


